import json
import logging
import os
import random
import re
import signal
import subprocess
import time
from datetime import datetime, timedelta
from json.decoder import JSONDecodeError
from typing import Tuple

LOGGER = logging.getLogger(__name__)


class S3bench:
    def __init__(self, access: str, secret: str, endpoint: str, bucket: str, object_prefix: str,
                 log_file: str, clients: int, samples: int, size_low: int, size_high: int,
                 head: bool, skip_read: bool, skip_write: bool, skip_cleanup: bool, validate: bool,
                 seed: int, duration: str = None, multipart: str = "0b"):
        """S3bench class for executing given s3bench workload

        S3bench workload tests generate following log files:
        1. {log_file}-report-i.log -> CLI redirection logs
        2. {log_file}-cli-i.log -> Generated by s3bench
        3. s3bench-{log_file}-i.log -> Generated by s3bench

        size_low: Object size in bytes
        size_high: Object size in bytes
        head: Head operation
        seed: Can be used to recreate the object size sequence, which is generated randomly
        duration: Duration string e.g. 100h0m, if not given will run for 100 days
        """
        random.seed(seed)
        if not self.install_s3bench():
            exit(-1)
        self.access_key = access
        self.secret_key = secret
        self.endpoint = endpoint
        self.bucket = bucket
        self.object_prefix = object_prefix
        self.num_clients = clients
        self.num_samples = samples
        self.label = log_file
        self.size_low = size_low
        self.size_high = size_high
        self.report_file = f"{self.label}-report"
        self.head = head
        self.skip_read = skip_read
        self.skip_write = skip_write
        self.skip_cleanup = skip_cleanup
        self.validate = validate
        self.multipart_size = multipart
        self.min_duration = 10
        if not duration:
            self.finish_time = datetime.now() + timedelta(hours=int(100 * 24))
        else:
            hours, minutes = duration.lower().replace("h", ":").replace("m", "").split(":")
            self.finish_time = datetime.now() + timedelta(hours=int(hours), minutes=int(minutes))
        self.cli_log = f"{self.label}-cli"
        self.cmd = None
        self.results = []
        self.errors = ["with error", "panic", "status code",
                       "flag provided but not defined", "InternalError", "ServiceUnavailable"]

    @staticmethod
    def install_s3bench():
        """Install s3bench if already not installed"""
        if os.system("s3bench --help"):
            LOGGER.error("ERROR: s3bench is not installed")
            if os.system(
                    "wget -O /usr/bin/s3bench https://github.com/Seagate/s3bench/releases/download/v2021-06-28/s3bench.2021-06-28"):
                LOGGER.error("ERROR: Unable to install go")
                return False
            if os.system("chmod +x /usr/bin/s3bench"):
                LOGGER.error("ERROR: Unable to install go")
                return False
            if os.system("s3bench --help"):
                LOGGER.error("ERROR: Unable to install s3bench")
                return False
        return True

    def execute_command(self, duration):
        """
        Execute s3bench command on local machine for given duration.
        Kill it after given duration if it is not complete
        :param duration: timeout.
        :return: bool: Subprocess killed due to timeout
        """
        LOGGER.info("Starting: %s wait: %s", self.cmd, duration)
        proc = subprocess.Popen(self.cmd, shell=True, preexec_fn=os.setsid)
        PGID = os.getpgid(proc.pid)
        counter = 0
        # Poll for either process completion or for timeout
        while counter < duration and proc.poll() is None:
            counter = counter + 5
            time.sleep(5)
        if proc.poll() is None:
            LOGGER.info("S3bench workload still running, Terminating.")
            os.killpg(PGID, signal.SIGKILL)
            return True
        else:
            LOGGER.info("S3bench workload is complete.")
            return False

    @staticmethod
    def delete_logs(logs):
        for log in logs:
            if os.path.exists(log):
                LOGGER.info("Deleting old log %s", log)
                os.remove(log)
            else:
                LOGGER.info("Old log %s does not exist", log)

    def execute_s3bench_workload(self):
        """Prepare and execute s3bench workload command"""
        i = 0
        while True:
            i = i + 1
            LOGGER.info("Iteration %s", i)
            iter_del = i - 5  # Iteration logs to be deleted
            if iter_del > 0:  # Delete logs
                old_report = f"{self.report_file}-{iter_del}.log"
                old_cli_log = f"{self.cli_log}-{iter_del}.log"
                old_log = f"s3bench-{self.label}-{iter_del}.log"
                self.delete_logs([old_report, old_cli_log, old_log])
            object_size = random.randrange(self.size_low, self.size_high)
            bucket = f"{self.bucket}-{i}-{time.time()}"
            cmd = f"s3bench -accessKey={self.access_key} -accessSecret={self.secret_key} " \
                  f"-endpoint={self.endpoint} -bucket={bucket} -objectSize={object_size}b " \
                  f"-numClients={self.num_clients} -numSamples={self.num_samples} " \
                  f"-objectNamePrefix={self.object_prefix} -t json "

            if self.head:
                cmd = cmd + "-headObj "
            if self.skip_write:
                cmd = cmd + "-skipWrite "
            if self.skip_read:
                cmd = cmd + "-skipRead "
            if self.skip_cleanup:
                cmd = cmd + "-skipCleanup "
            if self.validate:
                cmd = cmd + "-validate"

            report = f"{self.report_file}-{i}.log"
            cli_log = f"{self.cli_log}-{i}.log"
            label = f"{self.label}-{i}"
            self.cmd = f"{cmd} -o {report} -label {label} >> {cli_log} 2>&1"
            timedelta_v = (self.finish_time - datetime.now())
            timedelta_sec = timedelta_v.total_seconds()
            if timedelta_sec < self.min_duration:
                if i == 1:
                    LOGGER.info(f"s3bench workload did not execute since given duration is "
                                f"less than {self.min_duration} seconds.")
                    return False, None
                else:
                    LOGGER.info("s3bench workload execution done.")
                    return True, None
            LOGGER.info("Remaining test time %s", timedelta_v)
            timeout = self.execute_command(timedelta_sec)
            if timeout:
                LOGGER.info("Terminated s3bench workload due to timeout. Checking results.")
                return self.check_terminated_results(cli_log)
            else:
                status, ops = self.check_log_file_error(report, cli_log)
                if not status:
                    LOGGER.critical("Error found stopping execution")
                    return status, ops
                else:
                    LOGGER.info("No error found continuing execution")

    def check_log_file_error(self, report_file, cli_log) -> Tuple[bool, dict]:
        """
        Check if errors found in s3bench workload

        e.g. return: {"Write": 5, "Read":3, "Head":0}
        """
        try:
            report = json.load(open(report_file))
        except JSONDecodeError as e:
            LOGGER.error("Incorrect Json format %s - %s", report_file, e)
            return self.check_terminated_results(cli_log)
        ops = dict()
        error = True
        for test in report["Tests"]:
            operation = test["Operation"]
            errors = test["Errors Count"]
            ops[operation] = errors
            if test["Errors Count"] != 0:
                LOGGER.error(f"ERROR: {operation} operation failed with {errors} errors")
                error = False
            else:
                LOGGER.info(f"{operation} operation passed")
        return error, ops

    def check_terminated_results(self, cli_log):
        """Check results if s3bench workload is terminated"""
        pattern = fr"^M{0} \| [\d\/\.% \(\)]+ \| [a-z\d ]+ \| errors ([1-9]+)"
        ops = {"Write": 0, "Read": 0, "Validate": 0, "HeadObj": 0}
        error = True
        with open(cli_log) as log_f:
            data = log_f.read()
            errors_pattern = r"|".join(self.errors)
            matches = list(re.finditer(errors_pattern, data, re.MULTILINE))
            if len(matches):
                found_strings = set(x.group() for x in list(matches))
                LOGGER.error("S3bench workload failed with %s", found_strings)
                error = False
            for operation in ops:
                ops_pattern = pattern.format(operation)
                matches = re.finditer(ops_pattern, data, re.MULTILINE)
                matches = list(matches)
                if matches:
                    ops[operation] = matches[-1].group(1)
                    error = False
            return error, ops

    def run_check(self):
        """Execute s3bench workload & check failures in output"""
        return self.execute_s3bench_workload()
